{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применяем word2vec на практике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gensim** не зря считают библиотекой \"с человеческим лицом\" для topic modeling и vector semantics.\n",
    "\n",
    "Простой интерфейс и ПОРАЗИТЕЛЬНО высокая скорость обучения *word2vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np  \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(prefix, sentences, num_features=300, \n",
    "                   min_word_count=5, num_workers=4, \n",
    "                   context=10, downsampling=1e-3, save=True, sg=1):\n",
    "\n",
    "    # обучение\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    \n",
    "    model = Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count=min_word_count, \\\n",
    "                window=context, sample=downsampling, seed=1, sg=sg)\n",
    "\n",
    "    # сделаем модель поменьше в RAM\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    if save:\n",
    "        model_name = prefix + \"_\" + str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context\"\n",
    "        model.save(model_name)\n",
    "        print(\"Model\", model_name, \"saved\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Обучаем word2vec на наших текстах\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"Downloading\")\n",
    "\n",
    "# Толстой \n",
    "wp_txt = urllib.request.urlopen(\"https://www.gutenberg.org/files/2600/2600-h/2600-h.htm\")\n",
    "\n",
    "print(\"Parsing\")\n",
    "soup = BeautifulSoup(wp_txt)\n",
    "\n",
    "print(\"Cleaning\")\n",
    "wp_txt = soup.find('body').get_text()\n",
    "\n",
    "print(\"Downloading\")\n",
    "ak_txt = urllib.request.urlopen(\"http://www.gutenberg.org/files/1399/1399-0.txt\")\n",
    "ak_txt = ak_txt.read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "txt = wp_txt + \" \" + ak_txt\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Project Gutenberg EBook of Anna Karenina, by Leo Tolstoy\\n\\nThis eBook is for the use of anyone a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "txt = open(\"file1.txt\").read() + \" \" + open(\"file2.txt\").read()  + \" \" + open(\"file3.txt\").read()\n",
    "\n",
    "txt[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentences(txt, word_threshold=2, stage_train=True):\n",
    "\n",
    "    # вычищаем переносы\n",
    "    whitespaces = re.compile(\"\\s+\", re.U)\n",
    "    txt = re.sub(\"\\s+\", \" \", txt).lower()\n",
    "\n",
    "    # убираем всё, кроме \"слов\", разбив на предложения\n",
    "    sentences = re.split(\"[!\\?\\.]+\", txt.replace(\"\\n\", \" \"))\n",
    "    clean_sentences = [re.split(\"\\W+\", s) for s in sentences]\n",
    "    clean_sentences = [[w.replace(\"\\d+\", \"NUM\") for w in s if w] for s in clean_sentences]\n",
    "    \n",
    "    if stage_train:\n",
    "\n",
    "        counter = Counter()\n",
    "\n",
    "        for s in clean_sentences:\n",
    "            for w in s:\n",
    "                counter[w] += 1\n",
    "    \n",
    "        print(\"Filtered out word types :\", len([w for w in counter if counter[w] <= word_threshold]))\n",
    "        print(\"Filtered out words count:\", sum([counter[w] for w in counter if counter[w] <= word_threshold]))\n",
    "    \n",
    "        # выкидываем редкие, и заменяем их на специальный тег\n",
    "        clean_sentences = [[w if counter[w] > word_threshold else UNK for w in s] for s in clean_sentences]            \n",
    "    \n",
    "    word2index = { }\n",
    "    index2word = { }\n",
    "    \n",
    "    counter = max(word2index.values() if word2index else [0]) + 1\n",
    "\n",
    "    for s in clean_sentences:\n",
    "        for w in s:\n",
    "            if not w in word2index:\n",
    "                word2index[w] = counter\n",
    "                index2word[counter] = w\n",
    "                counter += 1\n",
    "                \n",
    "    return word2index, index2word, clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42638,\n",
       " 'the project gutenberg ebook of anna karenina by leo tolstoy this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index, index2word, clean_sentences = prepare_sentences(txt=txt, stage_train=False)\n",
    "\n",
    "len(clean_sentences), \" \".join(clean_sentences[:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model tolstoy__45features_10minwords_7context saved\n"
     ]
    }
   ],
   "source": [
    "w2v_model = train_word2vec(sentences=clean_sentences, \n",
    "                           prefix=\"tolstoy_\", \n",
    "                           context=7, downsampling=0.0001,\n",
    "                           min_word_count=10,\n",
    "                           num_features=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pdvlovna', 0.9271911382675171),\n",
       " ('countess', 0.9256846308708191),\n",
       " ('mikhdylovna', 0.9233927726745605),\n",
       " ('cess', 0.9135480523109436),\n",
       " ('natasha', 0.9118959307670593),\n",
       " ('mary', 0.9103429317474365),\n",
       " ('princess', 0.9097075462341309),\n",
       " ('daughter', 0.9034814238548279),\n",
       " ('sister', 0.9023302793502808),\n",
       " ('prin', 0.9015126824378967)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"anna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим, как можно оценивать качество\n",
    "Отличный источник, горячо рекомендуется\n",
    "https://github.com/EloiZ/embedding_evaluation\n",
    "\n",
    "Надо склонировать репозиторий, загрузить датасеты с помощью\n",
    "`download_benchmarks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# сохраним в CSV\n",
    "with open(\"tolstoy__100features_3minwords_5context.csv\", \"w+\") as wf:\n",
    "    writer = csv.writer(wf)\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        writer.writerow([word] + [v for v in  w2v_model.wv[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EMBEDDING_EVALUATION_DATA_PATH\"] = \"embedding_evaluation/data/\"\n",
    "\n",
    "import embedding_evaluation\n",
    "from embedding_evaluation.evaluate import Evaluation\n",
    "from embedding_evaluation.load_embedding import load_embedding_textfile\n",
    "\n",
    "def eval_word_vectors(path):\n",
    "    # Load embeddings as a dictionnary {word: embed} where embed is a 1-d numpy array.\n",
    "    embeddings = load_embedding_textfile(textfile_path=path)\n",
    "\n",
    "    # Load and process evaluation benchmarks\n",
    "    evaluation = Evaluation() \n",
    "\n",
    "    return evaluation.evaluate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concreteness': 0.31872708165611363,\n",
       " 'similarity': {'men': {'all_entities': 0.4217564105042674,\n",
       "   'entity_subset': 0},\n",
       "  'sem_sim': {'all_entities': 0.18682128976041365, 'entity_subset': 0},\n",
       "  'simlex': {'all_entities': 0.17514171209850715, 'entity_subset': 0},\n",
       "  'usf': {'all_entities': 0.15724184655559115, 'entity_subset': 0},\n",
       "  'vis_sim': {'all_entities': 0.1769497942246964, 'entity_subset': 0},\n",
       "  'ws353': {'all_entities': 0.16772503095065688, 'entity_subset': 0}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tolstoy = eval_word_vectors(\"tolstoy__100features_3minwords_5context.csv\")\n",
    "\n",
    "tolstoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Здесь везде -- more is better. Попробуйте настроить модель так, чтобы similarity выросла.\n",
    "\n",
    "Помогают ли советы?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравним с гугловскими векторами\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "\n",
    "`wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"`\n",
    "\n",
    "и распаковать (но лучше взять готовую модель у меня)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# загружаться может долго\n",
    "# w2v_ggl = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# filtered_w2v_ggl KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from  gensim.models import KeyedVectors\n",
    "# filtered_w2v_ggl = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  gensim.models import KeyedVectors\n",
    "\n",
    "filtered_w2v_ggl = KeyedVectors.load(\"filtered-GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews = eval_word_vectors(\"GoogleNews-vectors-negative300.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Значения метрик в читаемом виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concreteness 0.5719772752714409 0.31872708165611363\n",
      "similarity\n",
      "     usf\n",
      "       all_entities 0.3603241806749815 0.15724184655559115\n",
      "       entity_subset 0 0\n",
      "     sem_sim\n",
      "       all_entities 0.675366985636233 0.18682128976041365\n",
      "       entity_subset 0 0\n",
      "     ws353\n",
      "       all_entities 0.6873764284967581 0.16772503095065688\n",
      "       entity_subset 0 0\n",
      "     men\n",
      "       all_entities 0.7447273593551881 0.4217564105042674\n",
      "       entity_subset 0 0\n",
      "     vis_sim\n",
      "       all_entities 0.5934008833294928 0.1769497942246964\n",
      "       entity_subset 0 0\n",
      "     simlex\n",
      "       all_entities 0.3471553307624142 0.17514171209850715\n",
      "       entity_subset 0 0\n"
     ]
    }
   ],
   "source": [
    "for key in gnews.keys():\n",
    "    \n",
    "    if type(gnews[key]) != dict:\n",
    "        print(key, gnews[key], tolstoy[key])\n",
    "    else:\n",
    "        print(key)\n",
    "        for kk in gnews[key].keys():\n",
    "            if type(gnews[key][kk]) != dict:\n",
    "                print(\"    \", kk, gnews[key][kk], tolstoy[key][kk])\n",
    "            else:\n",
    "                print(\"    \", kk)\n",
    "                for kkk in gnews[key][kk].keys():\n",
    "                    if type(gnews[key][kk][kkk]) != dict:\n",
    "                        print(\"      \", kkk, gnews[key][kk][kkk], tolstoy[key][kk][kkk])\n",
    "                    else:\n",
    "                        print(\"      \", kkk, gnews[key][kk][kkk], tolstoy[key][kk][kkk])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec по-русски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нормализуем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2 \n",
    "from pymorphy2.tokenizers import *\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEMMATIZER = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "tags = re.compile(\"<[^>]*>\")\n",
    "html_codes = re.compile(\"&\\w+;\")\n",
    "nums = re.compile(\"\\d+\")\n",
    "nonalpha = re.compile(\"\\W+\", re.U)\n",
    "\n",
    "def remove_html(txt):\n",
    "    return html_codes.sub(\" \", tags.sub(\" \", txt))\n",
    "\n",
    "def replace_nums(txt):\n",
    "    return nums.sub(\"<num>\", txt)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [t for t in simple_word_tokenize(text) if not nonalpha.match(t)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1000000)\n",
    "def lemmatize(word):\n",
    "    p = LEMMATIZER.parse(word)[0]\n",
    "    return p.normal_form, p.tag\n",
    "\n",
    "\n",
    "def lemmatize_text(split_text):\n",
    "    return re.sub(\"\\s+\", \" \", \" \".join([lemmatize(t)[0] for t in split_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Parsing\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "print(\"Downloading\")\n",
    "\n",
    "# сжатый Толстой \n",
    "url = \"https://aldebaran.ru/author/tolstoyi_lev/kniga_anna_karenina1878_ru/download.html.zip\"\n",
    "ak_ru_zip = urllib.request.urlopen(url).read()\n",
    "\n",
    "with open(\"karenina.zip\", \"wb\") as wf:\n",
    "    wf.write(ak_ru_zip)\n",
    "\n",
    "with zipfile.ZipFile(\"karenina.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"karenina_ru\")\n",
    "\n",
    "print(\"Parsing\")\n",
    "\n",
    "html = open(\"karenina_ru/Tolstoyi_L._Anna_KareninaI.html\", encoding=\"windows-1251\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "print(\"Cleaning\")\n",
    "ak_ru_txt = soup.find('body').get_text()\n",
    "ak_ru_txt[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализуем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def prepare_sentences(text):\n",
    "\n",
    "    # список всех предложений в датасете\n",
    "    sentences_ru = []\n",
    "\n",
    "    review_sents = sentence_tokenizer.tokenize(replace_nums(remove_html(text)))\n",
    "    clean_sents = [lemmatize_text(tokenize(sentence)).split(\" \") for sentence in review_sents]\n",
    "\n",
    "    # список всех предложений в отзыве\n",
    "    sentences_ru.extend(clean_sents)\n",
    "    \n",
    "    return sentences_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak_sentences = prepare_sentences(ak_ru_txt)\n",
    "ak_sentences[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну такое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ak = train_word2vec(\"karenina_\", ak_sentences, num_features=45, context=5, min_word_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ak.similar_by_word(\"смерть\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация word2vec-ов\n",
    "способ снизить размерность и объединить синонимы в одну фичу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v_ak.wv.vocab.keys())\n",
    "w2v_matrix = np.array([w2v_ak.wv[key] for key in words])\n",
    "w2v_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "CLUSTERS = 200\n",
    "\n",
    "clusterer = MiniBatchKMeans(n_clusters=CLUSTERS, verbose=0, init_size=500, random_state=124, batch_size=10000)\n",
    "labels = clusterer.fit_predict(w2v_matrix)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем пустыми списками\n",
    "label2words = { label : [] for label in labels }\n",
    "words2label = {}\n",
    "i = 0\n",
    "\n",
    "for label in labels:\n",
    "    label2words[label].append(words[i])\n",
    "    words2label[words[i]] = label\n",
    "    i += 1\n",
    "    \n",
    "# распечатываем кластеры\n",
    "for label in label2words:\n",
    "    print(label)\n",
    "    print(\" \".join(label2words[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB: задачка для word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ggl = gensim.models.KeyedVectors.load(\"filtered-GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train = pd.read_csv('imdb_data/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('imdb_data/testData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv('imdb_data/unlabeledTrainData.tsv', header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review, remove_stops=True):   \n",
    "    \n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    words = letters_only.lower().split()     \n",
    "    \n",
    "    if remove_stops:\n",
    "        stops = set(stopwords.words(\"english\"))                 \n",
    "        return [w for w in words if not w in stops]  \n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    \n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        sentences = []\n",
    "\n",
    "        for raw_sentence in raw_sentences:        \n",
    "            if len(raw_sentence) > 0:\n",
    "                sentences.append(review_to_words(raw_sentence, remove_stopwords))\n",
    "                \n",
    "        return sentences\n",
    "\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "all_sentences_available = []\n",
    "\n",
    "for review in tqdm_notebook(train[\"review\"]):\n",
    "    all_sentences_available.extend(review_to_sentences(review, tokenizer, remove_stopwords=True))\n",
    "\n",
    "for review in tqdm_notebook(unlabeled_train[\"review\"]):\n",
    "    all_sentences_available.extend(review_to_sentences(review, tokenizer, remove_stopwords=True))\n",
    "\n",
    "len(all_sentences_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"all_sentences.bin\", \"wb\") as wf:\n",
    "#     pickle.dump(all_sentences_available, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_available = pickle.load(open(\"all_sentences.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model imdb_300features_5minwords_10context saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7ffa31637438>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_imdb = train_word2vec(\"imdb\", all_sentences_available)\n",
    "w2v_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"w2v_imdb\", \"wb\") as wf:\n",
    "#     pickle.dump(w2v_imdb, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_imdb = pickle.load(open(\"w2v_imdb\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начинаем готовить датасет для классификации по средним векторам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector_by_review(words, model):\n",
    "    \n",
    "    accumulator = np.zeros((model.vector_size,), dtype=\"float32\")\n",
    "    found_count = 0.\n",
    "    not_found = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            found_count += 1\n",
    "            accumulator +=  model.wv[word]\n",
    "        else:\n",
    "            not_found += 1\n",
    "    \n",
    "#     print(\"Not found percentage:\", not_found / (not_found + found_count))\n",
    "    \n",
    "    return accumulator / found_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vectors_for_dataset(reviews, model):\n",
    "    return np.matrix([avg_vector_by_review(review_to_words(review), model) for review in tqdm_notebook(reviews)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 4198/25000 [00:27<02:16, 152.02it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000, 300), (25000, 300))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs = avg_vectors_for_dataset(train[\"review\"], w2v_ggl)\n",
    "test_vecs = avg_vectors_for_dataset(test[\"review\"], w2v_ggl)\n",
    "\n",
    "train_vecs.shape, test_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будем предсказывать на СРЕДНИХ векторных представлениях слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main_train(model, param_grid, train_vecs, y, test_vecs):\n",
    "\n",
    "    # перебор гиперпараметров по сетке; по дефолту кросс-валидация StratifiedKFold\n",
    "    clf = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=7, verbose=1)\n",
    "\n",
    "    # отложим в сторону holdout, чтобы убедиться, \n",
    "    # что с нашими оценками на kfold всё в порядке\n",
    "    X_train, X_ho, y_train, y_ho = train_test_split(train_vecs, \n",
    "                                                    train.sentiment, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best score:\", clf.best_score_)\n",
    "    print(\"Best params:\", clf.best_params_)\n",
    "\n",
    "    # задаём модели лучшие найденные параметры\n",
    "    model = clf.best_estimator_\n",
    "\n",
    "    # обучаем на всём, кроме холдаута\n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "    # смотрим на качество предсказаний на холдауте\n",
    "    # должно быть похоже на оценку от поиска по сетке\n",
    "    print(\"Holdout score:\", model.score(X_ho, y_ho), \"-- is it close to the validation score?\")\n",
    "\n",
    "    # обучаем модель на всей размеченной выборке\n",
    "    model.fit(train_vecs, train.sentiment)\n",
    "    result = model.predict(test_vecs)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=7)]: Done  72 out of  72 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8514\n",
      "Best params: {'average': True, 'penalty': 'l2', 'loss': 'modified_huber'}\n",
      "Holdout score: 0.8554 -- is it close to the validation score?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(n_jobs=7)\n",
    "\n",
    "# надо ещё перебрать соотв. числа!\n",
    "penalties = [ \"l1\", \"l2\", \"elasticnet\"]\n",
    "losses = [ \"modified_huber\", \"log\", \"huber\", \"hinge\" ]\n",
    "\n",
    "# какие параметры рассмотрим\n",
    "param_grid = { \"penalty\": penalties, \"average\": [True, False], \"loss\": losses  }\n",
    "\n",
    "result = main_train(model, param_grid, train_vecs, train.sentiment, test_vecs)\n",
    "\n",
    "# запись сабмишшена для кегла\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"sgd-submission_wordvec_avg.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А теперь давайте использовать word2vec-и по-умному\n",
    "\n",
    "На основе многоканальной архитектуры Yoon Kim\n",
    "\n",
    "https://github.com/castorini/Castor/tree/master/kim_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class KimCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vectors, dropout_rate, input_channel, output_channel, target_class, maxlen):\n",
    "        \n",
    "        super(KimCNN, self).__init__()\n",
    "        \n",
    "        # число свёрток с разными окнами\n",
    "        Ks = 3 \n",
    "        \n",
    "        self.non_static_embed = nn.Embedding.from_pretrained(vectors, freeze=False)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channel, output_channel, ???, padding=(???, 0))\n",
    "        self.conv2 = nn.Conv1d(input_channel, output_channel, ???, padding=(???, 0))\n",
    "        self.conv3 = nn.Conv1d(input_channel, output_channel, ???, padding=(???, 0))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(Ks * output_channel, target_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        non_static_input = self.non_static_embed(x)\n",
    "        \n",
    "        print()\n",
    "        print(non_static_input.shape)\n",
    "        \n",
    "        # (batch, channel_input, sent_len, embed_dim)\n",
    "        x = non_static_input.unsqueeze(???) \n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # применяем свёртку к данным, и вычисляем функцию активации ReLU\n",
    "        x = [F.relu(self.conv1(x)), #.squeeze(???), \n",
    "             F.relu(self.conv2(x)), #.squeeze(???), \n",
    "             F.relu(self.conv3(x))] #.squeeze(???)]\n",
    "        \n",
    "        # (batch, channel_output, ~=sent_len) * Ks\n",
    "        x = [F.max_pool1d(i, i.size(???)).squeeze(???) for i in x] # max-over-time pooling\n",
    "        \n",
    "        # (batch, channel_output) * Ks\n",
    "        x = torch.cat(x, 1) \n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # (batch, target_size)\n",
    "        logit = self.fc1(x) \n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_by_review(words, model, word2id, max_len):\n",
    "    \n",
    "    accumulator = []\n",
    "    found_count = 0.\n",
    "    not_found = 0.\n",
    "    \n",
    "    for word in words[:max_len]:\n",
    "        if word in model.wv:\n",
    "            accumulator.append(word2id[word])\n",
    "        else:\n",
    "            accumulator.append(-1)\n",
    "    \n",
    "    for _ in range(0, max_len - len(words)):\n",
    "        accumulator.append(-1)\n",
    "    \n",
    "    accumulator = np.array(accumulator)\n",
    "    \n",
    "    return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим матрицу векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37065, 300)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "word2ix = {}\n",
    "\n",
    "for id, w in enumerate(filtered_w2v_ggl.wv.vocab):\n",
    "    embeddings.append(filtered_w2v_ggl.wv[w])\n",
    "    word2ix[w] = id\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "vocab_size = len(word2index)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "MAX_LEN = 50\n",
    "\n",
    "cnn_model = KimCNN(dropout_rate=0.4, \n",
    "                   input_channel=1, \n",
    "                   output_channel=1, \n",
    "                   target_class=1,\n",
    "                   vectors=torch.tensor(embeddings),\n",
    "                   maxlen=MAX_LEN).cuda()\n",
    "\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "def train_routine(model, loss_function, batches, epochs=30):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "\n",
    "        for features_batch, target_batch in tqdm_notebook(batches):\n",
    "\n",
    "            word_vectors_reviews = torch.tensor(features_batch, dtype=torch.long).cuda()\n",
    "\n",
    "            # градиенты надо сбрасывать, если не хотим аккумулировать\n",
    "            model.zero_grad()\n",
    "\n",
    "            # применяем модель\n",
    "            log_probs = model(word_vectors_reviews)\n",
    "\n",
    "            # вычисляем невязку\n",
    "            loss = loss_function(log_probs, torch.tensor(target_batch, dtype=torch.long).cuda())\n",
    "\n",
    "            # обратный проход, обновление градиента\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # получаем число\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        print(\"E\", epoch + 1, \"\\tNLL\\t\", total_loss / count)\n",
    "\n",
    "        losses.append(total_loss)\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l0, l1, n):\n",
    "    \n",
    "    assert len(l0) == len(l1)\n",
    "    coll0, coll1 = [], []\n",
    "    \n",
    "    for i in tqdm_notebook(range(0, len(l0), n)):\n",
    "        coll0.append(l0[i:i + n])\n",
    "        coll1.append(l1[i:i + n])\n",
    "        \n",
    "    return coll0, coll1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32326,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "          -1,    -1,    -1,    -1,    -1])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids_by_review(review_to_words(\"hello\"), filtered_w2v_ggl, word2ix, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [word_ids_by_review(review_to_words(txt), filtered_w2v_ggl, word2ix, MAX_LEN) for txt in train[\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_texts, batched_targets = chunks(data, list(train.sentiment), n=10)\n",
    "\n",
    "len(batched_texts), len(batched_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-9ac35f8fd013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-fd7aef0d7ebf>\u001b[0m in \u001b[0;36mtrain_routine\u001b[0;34m(model, loss_function, batches, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatures_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mword_vectors_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# градиенты надо сбрасывать, если не хотим аккумулировать\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "batches = list(zip(batched_texts, batched_targets))\n",
    "\n",
    "train_routine(cnn_model, loss_function, batches, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
