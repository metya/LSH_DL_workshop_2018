{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простые счётчики на службе векторной семантики\n",
    "## Скачиваем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'progressbar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b5bc9d367815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'progressbar'"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "with tqdm(...) as t:\n",
    "    text_data = request.urlopen(\"http://www.gutenberg.org/files/1399/1399-0.txt\").read().decode(\"UTF-8\") + \" \"\n",
    "    \n",
    "# with tqdm(...) as t:\n",
    "#     text_data += request.urlopen(\"https://archive.org/stream/warandpeace030164mbp/warandpeace030164mbp_djvu.txt\").read().decode(\"UTF-8\")\n",
    "\n",
    "#text_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Убираем теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('warandpeace.txt') as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to main content web texts movies audio software image logosearch Search upload personSIGN IN AB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "clean_data = re.sub(\"<[^>]*>\", \" \", text_data)\n",
    "clean_data = re.sub(\"\\s+\", \" \", text_data)\n",
    "clean_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бьём текст на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 36424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Skip to main content web texts movies audio software image logosearch Search upload personSIGN IN ABOUT CONTACT BLOG PROJECTS HELP DONATE JOBS VOLUNTEER PEOPLE Full text of \"War And Peace\" See other formats TEXT FLY WITHIN THE BOOK ONLY Tight Binding Book CO > UJ ft <OU_1 68052 >m OUP 2273 19-1 1-79 10,000 Copies.',\n",
       " \"OSMANIA UNIVERSITY LIBRARY Call No ^^L Accession No ^95 gg Author -j- 4.STt> y^ Title W<U *'<P4>*' ( *~' This book should bc^rcturned on or before the date last marked below.\",\n",
       " 'War and Peace BY LEO TOLSTOY Translated b\\\\ LOUISE and AYLMER MAUDE WILLIAM BENTON, Publisher ENCYCLOPEDIA BR1TANNICA, INC. CHICAGO - LONDON - TORONTO BY ARRANGEMENT WITH OXFORD UNIVERSITY PRESS COPYRIGHT IN THE UNITED STATES OF AMERICA, 1952, BY ENCYCLOPEDIA BRITANNICA,INC.',\n",
       " 'COPYRIGHT 1952.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "# загружаем токенизатор\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = sent_detector.tokenize(clean_data.strip())\n",
    "\n",
    "print(\"Total number of sentences:\", len(sentences))\n",
    "\n",
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The elderly lady was a Princess Drubet- skdya, belonging to one of the best families in Russia, but she was poor, and having long been out of society had lost her former influential connections.',\n",
       " 'She had now come to Petersburg to procure an appointment in the Guards for her only son.',\n",
       " \"It was, in fact, solely to meet Prince Vasfli that she had obtained an invita- tion to Anna Pdvlovna's reception and had sat listening to the vicomte's story.\",\n",
       " \"Prince Vasfli's words frightened her, an embittered look clouded her once handsome face, but only for a moment; then she smiled again and clutched Prince Vasili's arm more tightly.\",\n",
       " '\"Listen to me, Prince,\" said she.',\n",
       " '\"I have never yet asked you for anything and I never will again, nor have I ever reminded you of my father\\'s friendship for you; but now I en- treat you for God\\'s sake to do this for my son and I shall always regard you as a benefac- tor,\" she added hurriedly.',\n",
       " '\"No, don\\'t be angry, but promise!',\n",
       " 'I have asked Golitsyn and he has refused.',\n",
       " 'Be the kindhearted man you always were,\" she said, trying to smile though tears were in her eyes.',\n",
       " '\"Papa, we shall be late,\" said Princess Hel&ne, turning her beautiful head and look- ing over her classically molded shoulder as she stood waiting by the door.',\n",
       " 'Influence in society, however, is capital which has to be economized if it is to last.',\n",
       " 'Prince Vasfli knew this, and having once realized that if he asked on behalf of all who begged of him, he would soon be unable to ask for himself, he became chary of using his influ- ence.',\n",
       " \"But in Princess Drubetskdya's case he felt, after her second appeal, something like qualms of conscience.\",\n",
       " 'She had reminded him of what was quite true; he had been indebted to her father for the first steps in his career.',\n",
       " 'More- over, he could see by her manners that she was one of those womenmostly mothers who, having once made up their minds, will not rest until they have gained their end, and are pre- pared if necessary to go on insisting day after day and hour after hour, and even to make scenes.',\n",
       " 'This last consideration moved him.',\n",
       " '\"My dear Anna Mikhdylovna,\" said he with his usual familiarity and weariness of tone, \"it is almost impossible for me to do what you ask; but to prove my devotion to you and how I re- spect your father\\'s memory, I will do the im- possibleyour son shall be transferred to the Guards.',\n",
       " 'Here is my hand on it.',\n",
       " 'Are you satis- fied?\"',\n",
       " '* \"My dear benefactor!',\n",
       " 'This is what I ex- pected from you I knew your kindness!\"',\n",
       " 'He turned to go.',\n",
       " '\"Wait just a word!',\n",
       " 'When he has been trans- ferred to the Guards .',\n",
       " '.',\n",
       " '.\"',\n",
       " 'she faltered.',\n",
       " '\"You are on good terms with Michael Ilari6novich Kuttizov .',\n",
       " '.',\n",
       " '.',\n",
       " 'recommend Boris to him as adju- tant!',\n",
       " 'Then I shall be at rest, and then .',\n",
       " '.',\n",
       " '.\"',\n",
       " 'Prince Vasili smiled.',\n",
       " '\"No, I won\\'t promise that.',\n",
       " \"You don't know how Kutiizov is pestered since his appoint- ment as Commander in Chief.\",\n",
       " 'He told me himself that all the Moscow ladies have con- spired to give him all their sons as adjutants.\"',\n",
       " '\"No, but do promise!',\n",
       " \"I won't let you go!\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1000:1040]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиваем предложения на токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# можно попробовать и какой-нибудь другой\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# можно попробовать и какой-нибудь другой\n",
    "tokenizer = TweetTokenizer()\n",
    "splitted = []\n",
    "\n",
    "for sent in sentences:\n",
    "    splitted.append([lemmatizer.lemmatize(w).lower() for w in tokenizer.tokenize(sent) if re.match(\"^[A-Za-z'-]+$\", w)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Своеобразный способ построить словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix shape: (36424, 2077)\n",
      "Vocabulary samples: [('to', 1882), ('main', 1103), ('in', 943), ('about', 7), ('help', 861)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(' '), min_df=25)\n",
    "term_doc_matrix = vectorizer.fit_transform([\" \".join(sentence) for sentence in splitted])\n",
    "\n",
    "term2id = vectorizer.vocabulary_\n",
    "id2term = {v: k for k, v in term2id.items()}\n",
    "\n",
    "print(\"Term-document matrix shape:\", term_doc_matrix.shape)\n",
    "print(\"Vocabulary samples:\", list(term2id.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36424, 2077)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(term2id)\n",
    "term_doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посчитаем частоты термов, используя построенную матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2077,)\n"
     ]
    }
   ],
   "source": [
    "term_counts = term_doc_matrix.sum(axis=0).A1\n",
    "print(term_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "### Опциональное задание: выбросьте слишком частотные и слишком редкие термы\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: hint: надо использовать term_counts, term2id + перестроить все матрицы\n",
    "import matplotlib.pyplot as plt\n",
    "term_counts\n",
    "# plt.hist(term2id.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Строим term-context matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "half_window = 5\n",
    "\n",
    "X = np.zeros(shape=(len(term2id), len(term2id)))\n",
    "\n",
    "\n",
    "for sentence in splitted:\n",
    "    \n",
    "    # бежим с индексом по предложению\n",
    "    for i in range(len(sentence)):        \n",
    "        current_word = sentence[i]      \n",
    "        \n",
    "        # если слова нет в словаре, ничего для него не считаем\n",
    "        if current_word in term2id:\n",
    "            word_idx = term2id[current_word]\n",
    "\n",
    "            for c in range(-half_window, half_window):\n",
    "                current_idx = i + c\n",
    "\n",
    "                # проверяем, не наткнулись ли на границы предложения\n",
    "                if 0 <= current_idx < len(sentence):\n",
    "                    context_word = sentence[current_idx]\n",
    "                    \n",
    "                    if context_word in term2id:                    \n",
    "                        context_idx = term2id[context_word]\n",
    "                        X[word_idx, context_idx] += 1\n",
    "\n",
    "print(\"Sparsity of the term-context matrix\", len(X.nonzero()[0]) / (X.shape[0] ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция поиска ближайших K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def dict_k_closest(M, term_dict, inverse_term_dict, k=5):\n",
    "    \"\"\"\n",
    "        :param M -- матрица векторых представлений\n",
    "        :param term_dict -- слово2индекс\n",
    "        :param inverse_term_dict -- индекс2слово\n",
    "        :param k -- число ближайших соседей для выдачи\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Computing all distances... (takes some time)\")    \n",
    "    distances = cdist(M, M, \"cosine\")\n",
    "    sorted_by_dist_k = np.argsort(distances, axis=1)[:, :k]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for term in term_dict:\n",
    "        row_id = term_dict[term]\n",
    "        similar = [inverse_term_dict[i] for i in sorted_by_dist_k[row_id, :]]\n",
    "        results[term] = similar   \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Смотрим глазами на наши успехи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(X, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ну как вам?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Правильный ответ: не очень)\n",
    "\n",
    "\n",
    "### PMI: Pointwise Mutual Information\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PMI = X.copy()\n",
    "total_bicount = X_PMI.sum()\n",
    "total_unicount = term_counts.sum()\n",
    "\n",
    "# p(x,y) / p(x) / p(y)\n",
    "X_PMI = (X_PMI / total_bicount) / (term_counts[:, None] + 0.000001) / (term_counts[None, :] + 0.000001)  * total_unicount ** 2\n",
    "X_PMI = np.where(X_PMI > 0, np.log2(X_PMI), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(X_PMI, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PPMI = X_PMI.copy()\n",
    "X_PPMI[X_PPMI < 0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(X_PPMI, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ну как?\n",
    "Должно стать лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применим Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U, S, Vh = np.linalg.svd(X_PPMI, full_matrices=False)\n",
    "# U.shape, S.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "factorizer = TruncatedSVD(n_components=200, random_state=0)\n",
    "\n",
    "# матрица слов\n",
    "W = factorizer.fit_transform(X_PPMI)\n",
    "print(W.shape)\n",
    "\n",
    "# матрица контекстов\n",
    "C = factorizer.components_\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(W, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем разложить в произведение двух матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.decomposition import NMF\n",
    "\n",
    "factorizer = NMF(n_components=200, random_state=0)\n",
    "\n",
    "# матрица слов\n",
    "W = factorizer.fit_transform(X_PPMI)\n",
    "print(W.shape)\n",
    "\n",
    "# матрица контекстов\n",
    "C = factorizer.components_\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_terms = dict_k_closest(W, term2id, id2term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(similar_terms)[:10]:\n",
    "    print(key, \":\", \" \".join(similar_terms[key][1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Задача техническая: сохранить векторы в CSV\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим, как можно оценивать качество\n",
    "Отличный источник, горячо рекомендуется\n",
    "https://github.com/EloiZ/embedding_evaluation\n",
    "\n",
    "Надо склонировать репозиторий, загрузить датасеты с помощью\n",
    "`download_benchmarks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EMBEDDING_EVALUATION_DATA_PATH\"] = \"embedding_evaluation/data/\"\n",
    "\n",
    "import embedding_evaluation\n",
    "from embedding_evaluation.evaluate import Evaluation\n",
    "from embedding_evaluation.load_embedding import load_embedding_textfile\n",
    "\n",
    "def eval_word_vectors(path):\n",
    "    # Load embeddings as a dictionnary {word: embed} where embed is a 1-d numpy array.\n",
    "    embeddings = load_embedding_textfile(textfile_path=path)\n",
    "\n",
    "    # Load and process evaluation benchmarks\n",
    "    evaluation = Evaluation() \n",
    "\n",
    "    return evaluation.evaluate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача: оценить качество сохранённых моделей, попытаться его улучшить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodelresults = eval_word_vectors(\"trali-vali.csv\")\n",
    "\n",
    "mymodelresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skip',\n",
       " 'to',\n",
       " 'main',\n",
       " 'content',\n",
       " 'web',\n",
       " 'text',\n",
       " 'movie',\n",
       " 'audio',\n",
       " 'software',\n",
       " 'image',\n",
       " 'logosearch',\n",
       " 'search',\n",
       " 'upload',\n",
       " 'personsign',\n",
       " 'in',\n",
       " 'about',\n",
       " 'contact',\n",
       " 'blog',\n",
       " 'projects',\n",
       " 'help',\n",
       " 'donate',\n",
       " 'jobs',\n",
       " 'volunteer',\n",
       " 'people',\n",
       " 'full',\n",
       " 'text',\n",
       " 'of',\n",
       " 'war',\n",
       " 'and',\n",
       " 'peace',\n",
       " 'see',\n",
       " 'other',\n",
       " 'format',\n",
       " 'text',\n",
       " 'fly',\n",
       " 'within',\n",
       " 'the',\n",
       " 'book',\n",
       " 'only',\n",
       " 'tight',\n",
       " 'binding',\n",
       " 'book',\n",
       " 'co',\n",
       " 'uj',\n",
       " 'ft',\n",
       " 'm',\n",
       " 'oup',\n",
       " 'copies']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
