{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковые модели \n",
    "### из эпохи до того момента, когда нейронные сети вдруг стали диплёрнингом\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Downloading\")\n",
    "\n",
    "# with tqdm(...) as t:\n",
    "txt = urllib.request.urlopen(\"https://www.gutenberg.org/files/2600/2600-h/2600-h.htm\").read().decode(\"utf-8\")\n",
    "txt2 = urllib.request.urlopen(\"http://www.gutenberg.org/files/36028/36028-h/36028-h.htm\").read().decode(\"utf-8\")\n",
    "\n",
    "txt = str(txt) + \" \" + str(txt2)\n",
    "\n",
    "print(\"Parsing\")\n",
    "soup = BeautifulSoup(txt)\n",
    "\n",
    "print(\"Cleaning\")\n",
    "txt = soup.find('body').get_text()\n",
    "\n",
    "print(txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "BOS = \"BOS\"\n",
    "EOS = \"EOS\"\n",
    "UNK = \"UNK\"\n",
    "\n",
    "def prepare_sentences(txt, word_threshold=2, stage_train=True):\n",
    "\n",
    "    # вычищаем переносы\n",
    "    whitespaces = re.compile(\"\\s+\", re.U)\n",
    "    txt = re.sub(\"\\s+\", \" \", txt).lower()\n",
    "\n",
    "    # убираем всё, кроме \"слов\", разбив на предложения\n",
    "    sentences = re.split(\"[!\\?\\.]+\", txt.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    # оставляем только alphanumeric\n",
    "    clean_sentences = [re.split(\"\\W+\", s) for s in sentences]\n",
    "    \n",
    "    # заменяем числа на NUM\n",
    "    clean_sentences = [[w.replace(\"\\d+\", \"NUM\") for w in s if w] for s in clean_sentences]\n",
    "    \n",
    "    # вводим тег UNKNOWN: UNK\n",
    "    if stage_train:\n",
    "\n",
    "        counter = Counter()\n",
    "\n",
    "        for s in clean_sentences:\n",
    "            for w in s:\n",
    "                counter[w] += 1\n",
    "    \n",
    "        print(\"Filtered out word types :\", len([w for w in counter if counter[w] <= word_threshold]))\n",
    "        print(\"Filtered out words count:\", sum([counter[w] for w in counter if counter[w] <= word_threshold]))\n",
    "    \n",
    "        # выкидываем редкие, и заменяем их на специальный тег\n",
    "        clean_sentences = [[w if counter[w] > word_threshold else UNK for w in s] for s in clean_sentences]            \n",
    "    \n",
    "    word2index = { BOS: 0, EOS: 1, UNK: 2}\n",
    "    index2word = { 0: BOS, 1: EOS, 2: UNK}\n",
    "    \n",
    "    counter = max(word2index.values()) + 1\n",
    "\n",
    "    for s in clean_sentences:\n",
    "        for w in s:\n",
    "            if not w in word2index:\n",
    "                word2index[w] = counter\n",
    "                index2word[counter] = w\n",
    "                counter += 1\n",
    "                \n",
    "    return word2index, index2word, clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, clean_sentences = prepare_sentences(txt)\n",
    "\n",
    "print(\"Total number of sentences :\\t\", len(clean_sentences))\n",
    "print(\"Total number of words     :\\t\", sum([len(sent) for sent in clean_sentences]))\n",
    "print(\"Total number of word types:\\t\", len(set([w for sent in clean_sentences for w in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(sentence, context_size):\n",
    "    \"\"\"\n",
    "        Добиваем символы начала и конца строки к каждому предложению\n",
    "    \"\"\"\n",
    "    return [BOS] * context_size + sentence + [EOS] * context_size\n",
    "\n",
    "def enumerate_sentences(clean_sentences, context_size, word2index):\n",
    "    \"\"\"\n",
    "        Добиваем символами начала и конца и конвертируем слова в индексы\n",
    "    \"\"\"\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    UNK_id = word2index[UNK]\n",
    "\n",
    "    for sentence in clean_sentences:\n",
    "\n",
    "        aligned_sentence =  augment(sentence, context_size) \n",
    "\n",
    "        for i in range(context_size, len(sentence) - context_size, 1):\n",
    "            \n",
    "            # берём предшествующий контекст\n",
    "            context = aligned_sentence[i - context_size:i]\n",
    "            context = [word2index[c] if c in word2index else UNK_id for c in context]\n",
    "            target = word2index[aligned_sentence[i]] if aligned_sentence[i] in word2index else UNK_id\n",
    "            \n",
    "            contexts.append(context)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return contexts, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как бить на батчи заданного размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l0, l1, n):\n",
    "    \n",
    "    assert len(l0) == len(l1)\n",
    "    coll0, coll1 = [], []\n",
    "    \n",
    "    for i in range(0, len(l0), n):\n",
    "        coll0.append(l0[i:i + n])\n",
    "        coll1.append(l1[i:i + n])\n",
    "        \n",
    "    return coll0, coll1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import lru_cache\n",
    "\n",
    "class NGramFreqsLanguageModeler(object):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_size):\n",
    "        super(NGramFreqsLanguageModeler, self).__init__()\n",
    "    \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.ngram_dict = defaultdict(lambda: defaultdict(lambda: 0))        \n",
    "        self.n_1_gram_dict = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.contexts_counts = defaultdict(lambda: 0)\n",
    "        self.eps = 1.0\n",
    "    \n",
    "    def fit(self, contexts, targets):\n",
    "        \n",
    "        self.contexts_counts = defaultdict(lambda: 0)\n",
    "        \n",
    "        for c, t in zip(contexts, targets):\n",
    "            c = tuple(c)\n",
    "            self.ngram_dict[c][t] += 1\n",
    "            self.contexts_counts[c] += 1\n",
    "            \n",
    "            # намёк!\n",
    "            # self.n_1_gram_dict[c[1:]][t] += 1\n",
    "\n",
    "            \n",
    "        print(\"Total n-1 grams\", len(self.ngram_dict), list(self.ngram_dict)[:10])\n",
    "        \n",
    "        # нормализуем частоты\n",
    "        for c in tqdm_notebook(self.ngram_dict.keys()):\n",
    "            for t in self.ngram_dict[c]:\n",
    "                self.ngram_dict[c][t] = (self.ngram_dict[c][t] +  self.eps) / \\\n",
    "                                            (self.contexts_counts[c] + self.vocab_size * self.eps)\n",
    "        \n",
    "    @lru_cache(1000000)\n",
    "    def prob_dist(self, input_context):\n",
    "        \"\"\"\n",
    "            Takes ngram as a tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = np.zeros(self.vocab_size) + \\\n",
    "                    self.eps / (self.vocab_size * self.eps + self.contexts_counts[input_context])\n",
    "        \n",
    "        counts = self.ngram_dict[input_context]\n",
    "        \n",
    "        # если есть хоть какие-то счётчики\n",
    "        if counts:\n",
    "            \n",
    "            # проставим осмысленные частоты\n",
    "            for target, freq in counts.items():\n",
    "                probs[target] = freq\n",
    "                \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "BATCH_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "\n",
    "# строим контексты и цели\n",
    "contexts, targets = enumerate_sentences(clean_sentences, CONTEXT_SIZE, word2index)\n",
    "\n",
    "batches = list(zip(contexts, targets))\n",
    "\n",
    "simple_model = NGramFreqsLanguageModeler(context_size=CONTEXT_SIZE, vocab_size=len(word2index))\n",
    "simple_model.fit(contexts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества\n",
    "\n",
    "здесь мы готовим тестовую выборку -- тексты, которых наша модель никогда не видела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "test_txt = urllib.request.urlopen(\"http://www.gutenberg.org/files/1399/1399-0.txt\")\n",
    "test_txt = test_txt.read().decode(\"utf-8\")\n",
    "\n",
    "_, _, test_clean_sentences = prepare_sentences(test_txt, stage_train=False)\n",
    "\n",
    "print(\"Total number of sentences :\\t\", len(test_clean_sentences))\n",
    "print(\"Total number of words     :\\t\", sum([len(sent) for sent in test_clean_sentences]))\n",
    "print(\"Total number of word types:\\t\", len(set([w for sent in test_clean_sentences for w in sent])))\n",
    "\n",
    "# строим контексты и цели\n",
    "test_contexts, test_targets = enumerate_sentences(test_clean_sentences, CONTEXT_SIZE, word2index)\n",
    "\n",
    "# test_data = list(zip(test_contexts, test_targets))\n",
    "\n",
    "test_batched_contexts, test_batched_targets = chunks(test_contexts, test_targets, BATCH_SIZE)\n",
    "test_batches = list(zip(test_batched_contexts, test_batched_targets))\n",
    "\n",
    "len(test_contexts), len(test_targets), len(test_batches), len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перплексия \n",
    "То, насколько хорошо наша модель приближает законы реального мира: какую вероятность порождения тестового текста, нормализованную числом слов, покажет наша модель.\n",
    "\n",
    "Имеет трактовку с точки зрения теории информации: два в степени, равной приближению кросс-энтропии последовательности событий-слов. То есть такая оценка энтропии текста как последовательности.\n",
    "\n",
    "#### это два в степени равной нашей невязке\n",
    "\n",
    "$$PP(X) = \\sqrt[N]{\\frac{1}{P(x_1,...,x_N)}} = 2^{-\\frac{1}{N}\\sum_{i=1}^{N}\\log{P(x_i|...)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def compute_ppl_count_model(model, test_batches, nllloss):\n",
    "    \n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for context_batch, target_batch in tqdm_notebook(test_batches):\n",
    "        \n",
    "        log_probs = []\n",
    "        \n",
    "        for context, target in zip(context_batch, target_batch):\n",
    "            \n",
    "            # применяем модель\n",
    "            log_probs.append(np.log2(model.prob_dist(tuple(context))))\n",
    "            \n",
    "        log_probs = np.array(log_probs)\n",
    "        \n",
    "        # вычисляем невязку\n",
    "        loss = nllloss(torch.tensor(log_probs, dtype=torch.float).cuda(), \n",
    "                       torch.tensor(target_batch, dtype=torch.long).cuda())\n",
    "        \n",
    "        # получаем число\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        \n",
    "        if count % (len(test_batches) // 5) == 0:\n",
    "            print(count, \"\\tnll\", total_loss)\n",
    "            print([index2word[i] for i in context_batch[0]], \"->\", \n",
    "                  index2word[target_batch[0]], \"vs\", \n",
    "                  [index2word[i] for i in (-log_probs[0]).argsort()[:3]],\n",
    "                  np.sort((-log_probs[0]))[:3]\n",
    "                 )\n",
    "    \n",
    "    return 2 ** (total_loss / count)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss\n",
    "\n",
    "\"Perplexity of freq-based NGram model on test set\", compute_ppl_count_model(model=simple_model, \n",
    "                                                                            nllloss=NLLLoss(),\n",
    "                                                                            test_batches=test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пусть наша модель погенерирует что-нибудь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"BOS\"\n",
    "prepared_text = augment(prepare_sentences(test, stage_train=False)[2][0], CONTEXT_SIZE)[-CONTEXT_SIZE:]\n",
    "\n",
    "for i in range(CONTEXT_SIZE, 10 + CONTEXT_SIZE):\n",
    "    \n",
    "    idx = [word2index[w] for w in prepared_text[:i]]    \n",
    "    \n",
    "    predict = simple_model.prob_dist(tuple(idx[-CONTEXT_SIZE:])) \n",
    "    \n",
    "#     predict = predict - predict.min()  \n",
    "#     predict /= sum(predict)\n",
    "    \n",
    "    selected_word = np.random.choice(a=list(range(len(word2index))), p=predict)    \n",
    "    prepared_text.append(index2word[selected_word])\n",
    "    \n",
    "    print(\"Генерация:\", \" \".join(prepared_text[CONTEXT_SIZE - 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Задание 2: применить трюки из лекции (настройка Лапласа? backoff?)\n",
    "----\n",
    "Прикрутите его, пожалуйста, и вычислите перплексию на тестовом множестве\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Большая задача: kaggle\n",
    "\n",
    "1. самостоятельно подготовить данные для обучения\n",
    "2. обучить модель по train.tsv\n",
    "3. сравнить два текста; какой из них естественнее, тот и молодец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
